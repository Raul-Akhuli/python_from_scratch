# Floating-point numbers are a broader category that includes
# various formats for representing real numbers, while "float" 
# typically refers to a specific type of floating-point number, 
# usually single-precision (32-bit) in programming.
# Floats have limited precision and range compared to 
# double-precision (64-bit) floating-point numbers,
# which can represent values more accurately.



# float - 32 bit, single precision.
# floating point numbers - 64 bit, double precision.